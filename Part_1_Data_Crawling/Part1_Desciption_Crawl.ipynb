{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from csv import writer\n",
    "from tqdm import tqdm, trange\n",
    "from string import digits\n",
    "from os.path import exists\n",
    "from random import randrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityData:\n",
    "    def __init__(self,name,state,title,description):\n",
    "        self.name = name\n",
    "        self.state = state\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "    def write_single_csv(self):\n",
    "        filename = './data/' + self.state + '_documents.csv'\n",
    "        if not exists(filename):\n",
    "            with open(filename, \"x\") as file:\n",
    "                file.write(\"Name,State,Title,Description\\n\")\n",
    "        with open(filename, \"a\", encoding='utf-8') as file:\n",
    "            file.write(self.info())\n",
    "            \n",
    "    def write_whole_csv(self):\n",
    "        filename = 'documents.csv'\n",
    "        if not exists(filename):\n",
    "            with open(filename, \"x\") as file:\n",
    "                file.write(\"Name,State,Title,Description\\n\")\n",
    "        with open(filename, \"a\", encoding='utf-8') as file:\n",
    "            file.write(self.info())\n",
    "\n",
    "    def info(self):\n",
    "        rep = '\"' + '\",\"'.join([self.name, self.state, self.title, self.description]) + '\"\\n'    \n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stateurl(source_url,soup):\n",
    "    #get url linked to each state\n",
    "    urls=soup.select('div[id=\"tabs_by_category\"]>ul[class=\"tab-list tab-list-short\"] > li > a')\n",
    "    state_urls={url['href'].split('/')[-1].split('.')[0]:'https:' + url['href'] for url in urls}\n",
    "    return state_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"https://www.city-data.com/articles/\"\n",
    "#tabs_by_category\n",
    "response = requests.get(source).text\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "state_urls=get_stateurl(source,soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/77 [14:06<2:01:41, 105.82s/it]\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(0 bytes read, 8096 more expected)', IncompleteRead(0 bytes read, 8096 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:443\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:818\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    817\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_chunk(amt)\n\u001b[0;32m    819\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m    820\u001b[0m     chunk, decode_content\u001b[39m=\u001b[39mdecode_content, flush_decoder\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    821\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:771\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m     returned_chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_left)\n\u001b[0;32m    772\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39m_safe_read(\u001b[39m2\u001b[39m)  \u001b[39m# Toss the CRLF at the end of the chunk.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2032.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:632\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m--> 632\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read, 8096 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:623\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 623\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content):\n\u001b[0;32m    624\u001b[0m         \u001b[39myield\u001b[39;00m line\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:803\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[39mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[0;32m    799\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    800\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m     )\n\u001b[1;32m--> 803\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m    804\u001b[0m     \u001b[39m# Don't bother reading the body of a HEAD request.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response \u001b[39mand\u001b[39;00m is_response_to_head(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2032.0_x64__qbz5n2kfra8p0\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\response.py:460\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m (HTTPException, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# This includes IncompleteRead.\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection broken: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e, e)\n\u001b[0;32m    462\u001b[0m \u001b[39m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39m# unnecessarily.\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read, 8096 more expected)', IncompleteRead(0 bytes read, 8096 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.city-data.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m site_url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.city-data.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m site_url[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m], stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m r:\u001b[38;5;66;03m# to avoid error\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     value\u001b[38;5;241m=\u001b[39m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[0;32m     14\u001b[0m site_dict[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# add a gap to prevent being blocked\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:923\u001b[0m, in \u001b[0;36mResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    920\u001b[0m content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    921\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding\n\u001b[1;32m--> 923\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontent:\n\u001b[0;32m    924\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    926\u001b[0m \u001b[39m# Fallback to auto-detected encoding.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    819\u001b[0m \u001b[39mexcept\u001b[39;00m DecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read, 8096 more expected)', IncompleteRead(0 bytes read, 8096 more expected))"
     ]
    }
   ],
   "source": [
    "\n",
    "for state_page_name, state_url in tqdm(state_urls.items()):\n",
    "    filename = './cache/' + state_page_name + '.json'\n",
    "    if  not exists(filename):\n",
    "        with requests.get(state_url, stream=True) as x:\n",
    "            response=x.text\n",
    "            soup = BeautifulSoup(response, 'html.parser')\n",
    "    # read the site pages\n",
    "            site_urls = soup.select('div[id=\"listing\"] > div[class=\"col-md-4\"] > a')\n",
    "            site_dict = {}\n",
    "            for site_url in site_urls:\n",
    "                key = 'https://www.city-data.com' + site_url['href']\n",
    "                with requests.get('https://www.city-data.com' + site_url['href'], stream=True) as r:# to avoid error\n",
    "                    value=r.text\n",
    "                    site_dict[key] = value\n",
    "                time.sleep(0.1) # add a gap to prevent being blocked\n",
    "        filename = './cache/' + state_page_name + '.json'\n",
    "        with open(filename, 'w+') as json_file:\n",
    "            json.dump(site_dict, json_file, indent=4)\n",
    "        time.sleep(300)\n",
    "   \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_site_page(state_page_name, site_url):\n",
    "    # helper function: get response of a site page, return None if not exist\n",
    "    filename = './cache/' + state_page_name + '.json'\n",
    "    try:\n",
    "        with open(filename) as json_file:\n",
    "            site_dict = json.load(json_file)\n",
    "            response = site_dict[site_url]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_site_instance(state_page_name, site_url):\n",
    "    # get the site instance for a site page from a state_page_name and site_url\n",
    "    response = get_cache_site_page(state_page_name, site_url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    state = state_page_name.translate(str.maketrans('', '', digits))\n",
    "    title = soup.select('h1[class=\"city\"] > span')[0].contents[0]\n",
    "    paragraphs = [p.contents[0] for p in soup.select('div[class=\"well\"] > p') if not p.find_all()]\n",
    "    description = ' '.join(paragraphs)\n",
    "    if title.count(' - ') == 2:\n",
    "        name = title.split(' - ')[0]\n",
    "    elif title.count(', ') == 2:\n",
    "        name = title.split(', ')[0]\n",
    "    else:\n",
    "        name = title\n",
    "\n",
    "    touristsite = CityData(name.replace('\"', \"\"),\n",
    "                              state.replace('\"', \"\"), \n",
    "                              title.replace('\"', \"\"), \n",
    "                              description.replace('\"', \"\"))\n",
    "    \n",
    "    return touristsite\n",
    "def get_cache_sites_for_state_page(state_page_name):\n",
    "    # get a list of site urls given the state page\n",
    "    filename = './cache/' + state_page_name + '.json'\n",
    "    try:\n",
    "        with open(filename) as json_file:\n",
    "            site_dict = json.load(json_file)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    site_list = list(site_dict.keys())\n",
    "    return site_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START WRITING FILES TO DOC DATA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Alaska> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [01:17<00:00,  2.82it/s]\n",
      "  1%|▏         | 1/77 [01:18<1:38:56, 78.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Alabama> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:29<00:00,  4.37it/s]\n",
      "  3%|▎         | 2/77 [01:48<1:02:11, 49.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Arkansas> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:45<00:00,  3.59it/s]\n",
      "  4%|▍         | 3/77 [02:34<59:35, 48.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Arizona> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:17<00:00,  2.18it/s]\n",
      "  5%|▌         | 4/77 [04:52<1:41:53, 83.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Arizona2> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:18<00:00,  2.17it/s]\n",
      "  6%|▋         | 5/77 [07:15<2:06:11, 105.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <Arizona3> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s]\n",
      "  8%|▊         | 6/77 [08:02<1:40:54, 85.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <California> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:17<00:00,  2.19it/s]\n",
      "  9%|▉         | 7/77 [10:25<2:01:25, 104.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <California2> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:14<00:00,  2.23it/s]\n",
      "100%|██████████| 77/77 [12:44<00:00,  9.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sites from <California3> to csv file...\n",
      "Writing sites from <California4> to csv file...\n",
      "Writing sites from <California5> to csv file...\n",
      "Writing sites from <California6> to csv file...\n",
      "Writing sites from <California7> to csv file...\n",
      "Writing sites from <California8> to csv file...\n",
      "Writing sites from <California9> to csv file...\n",
      "Writing sites from <California10> to csv file...\n",
      "Writing sites from <California11> to csv file...\n",
      "Writing sites from <Colorado> to csv file...\n",
      "Writing sites from <Colorado2> to csv file...\n",
      "Writing sites from <Connecticut> to csv file...\n",
      "Writing sites from <District-of-Columbia> to csv file...\n",
      "Writing sites from <Delaware> to csv file...\n",
      "Writing sites from <Florida> to csv file...\n",
      "Writing sites from <Florida2> to csv file...\n",
      "Writing sites from <Florida3> to csv file...\n",
      "Writing sites from <Florida4> to csv file...\n",
      "Writing sites from <Florida5> to csv file...\n",
      "Writing sites from <Florida6> to csv file...\n",
      "Writing sites from <Georgia> to csv file...\n",
      "Writing sites from <Georgia2> to csv file...\n",
      "Writing sites from <Hawaii> to csv file...\n",
      "Writing sites from <Hawaii2> to csv file...\n",
      "Writing sites from <Hawaii3> to csv file...\n",
      "Writing sites from <Iowa> to csv file...\n",
      "Writing sites from <Idaho> to csv file...\n",
      "Writing sites from <Illinois> to csv file...\n",
      "Writing sites from <Indiana> to csv file...\n",
      "Writing sites from <Kansas> to csv file...\n",
      "Writing sites from <Kentucky> to csv file...\n",
      "Writing sites from <Louisiana> to csv file...\n",
      "Writing sites from <Massachusetts> to csv file...\n",
      "Writing sites from <Maryland> to csv file...\n",
      "Writing sites from <Maine> to csv file...\n",
      "Writing sites from <Michigan> to csv file...\n",
      "Writing sites from <Minnesota> to csv file...\n",
      "Writing sites from <Missouri> to csv file...\n",
      "Writing sites from <Mississippi> to csv file...\n",
      "Writing sites from <Montana> to csv file...\n",
      "Writing sites from <North-Carolina> to csv file...\n",
      "Writing sites from <North-Dakota> to csv file...\n",
      "Writing sites from <Nebraska> to csv file...\n",
      "Writing sites from <New-Hampshire> to csv file...\n",
      "Writing sites from <New-Jersey> to csv file...\n",
      "Writing sites from <New-Mexico> to csv file...\n",
      "Writing sites from <Nevada> to csv file...\n",
      "Writing sites from <Nevada2> to csv file...\n",
      "Writing sites from <New-York> to csv file...\n",
      "Writing sites from <New-York2> to csv file...\n",
      "Writing sites from <New-York3> to csv file...\n",
      "Writing sites from <Ohio> to csv file...\n",
      "Writing sites from <Oklahoma> to csv file...\n",
      "Writing sites from <Oregon> to csv file...\n",
      "Writing sites from <Pennsylvania> to csv file...\n",
      "Writing sites from <Pennsylvania2> to csv file...\n",
      "Writing sites from <Rhode-Island> to csv file...\n",
      "Writing sites from <South-Carolina> to csv file...\n",
      "Writing sites from <South-Dakota> to csv file...\n",
      "Writing sites from <Tennessee> to csv file...\n",
      "Writing sites from <Texas> to csv file...\n",
      "Writing sites from <Texas2> to csv file...\n",
      "Writing sites from <Utah> to csv file...\n",
      "Writing sites from <Virginia> to csv file...\n",
      "Writing sites from <Vermont> to csv file...\n",
      "Writing sites from <Washington> to csv file...\n",
      "Writing sites from <Wisconsin> to csv file...\n",
      "Writing sites from <West-Virginia> to csv file...\n",
      "Writing sites from <Wyoming> to csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#write data to csv files:\n",
    "# write to the csv file\n",
    "print(\"--- START WRITING FILES TO DOC DATA ---\")\n",
    "count=0\n",
    "for state_page_name, state_url in tqdm(state_urls.items()):\n",
    "    count+=1\n",
    "    site_list = get_cache_sites_for_state_page(state_page_name)\n",
    "    print(f\"Writing sites from <{state_page_name}> to csv file...\")\n",
    "    filename = './data/' + state_page_name + '_documents.csv'\n",
    "    if not exists(filename) and count<=8:\n",
    "        for site_url in tqdm(site_list):\n",
    "            try:\n",
    "                site = get_site_instance(state_page_name, site_url)\n",
    "                site.write_single_csv()\n",
    "                site.write_whole_csv()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(source_url,soup):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alaska': 'https://www.city-data.com/articles/Alaska.html',\n",
       " 'Alabama': 'https://www.city-data.com/articles/Alabama.html',\n",
       " 'Arkansas': 'https://www.city-data.com/articles/Arkansas.html',\n",
       " 'Arizona': 'https://www.city-data.com/articles/Arizona.html',\n",
       " 'Arizona2': 'https://www.city-data.com/articles/Arizona2.html',\n",
       " 'Arizona3': 'https://www.city-data.com/articles/Arizona3.html',\n",
       " 'California': 'https://www.city-data.com/articles/California.html',\n",
       " 'California2': 'https://www.city-data.com/articles/California2.html',\n",
       " 'California3': 'https://www.city-data.com/articles/California3.html',\n",
       " 'California4': 'https://www.city-data.com/articles/California4.html',\n",
       " 'California5': 'https://www.city-data.com/articles/California5.html',\n",
       " 'California6': 'https://www.city-data.com/articles/California6.html',\n",
       " 'California7': 'https://www.city-data.com/articles/California7.html',\n",
       " 'California8': 'https://www.city-data.com/articles/California8.html',\n",
       " 'California9': 'https://www.city-data.com/articles/California9.html',\n",
       " 'California10': 'https://www.city-data.com/articles/California10.html',\n",
       " 'California11': 'https://www.city-data.com/articles/California11.html',\n",
       " 'Colorado': 'https://www.city-data.com/articles/Colorado.html',\n",
       " 'Colorado2': 'https://www.city-data.com/articles/Colorado2.html',\n",
       " 'Connecticut': 'https://www.city-data.com/articles/Connecticut.html',\n",
       " 'District-of-Columbia': 'https://www.city-data.com/articles/District-of-Columbia.html',\n",
       " 'Delaware': 'https://www.city-data.com/articles/Delaware.html',\n",
       " 'Florida': 'https://www.city-data.com/articles/Florida.html',\n",
       " 'Florida2': 'https://www.city-data.com/articles/Florida2.html',\n",
       " 'Florida3': 'https://www.city-data.com/articles/Florida3.html',\n",
       " 'Florida4': 'https://www.city-data.com/articles/Florida4.html',\n",
       " 'Florida5': 'https://www.city-data.com/articles/Florida5.html',\n",
       " 'Florida6': 'https://www.city-data.com/articles/Florida6.html',\n",
       " 'Georgia': 'https://www.city-data.com/articles/Georgia.html',\n",
       " 'Georgia2': 'https://www.city-data.com/articles/Georgia2.html',\n",
       " 'Hawaii': 'https://www.city-data.com/articles/Hawaii.html',\n",
       " 'Hawaii2': 'https://www.city-data.com/articles/Hawaii2.html',\n",
       " 'Hawaii3': 'https://www.city-data.com/articles/Hawaii3.html',\n",
       " 'Iowa': 'https://www.city-data.com/articles/Iowa.html',\n",
       " 'Idaho': 'https://www.city-data.com/articles/Idaho.html',\n",
       " 'Illinois': 'https://www.city-data.com/articles/Illinois.html',\n",
       " 'Indiana': 'https://www.city-data.com/articles/Indiana.html',\n",
       " 'Kansas': 'https://www.city-data.com/articles/Kansas.html',\n",
       " 'Kentucky': 'https://www.city-data.com/articles/Kentucky.html',\n",
       " 'Louisiana': 'https://www.city-data.com/articles/Louisiana.html',\n",
       " 'Massachusetts': 'https://www.city-data.com/articles/Massachusetts.html',\n",
       " 'Maryland': 'https://www.city-data.com/articles/Maryland.html',\n",
       " 'Maine': 'https://www.city-data.com/articles/Maine.html',\n",
       " 'Michigan': 'https://www.city-data.com/articles/Michigan.html',\n",
       " 'Minnesota': 'https://www.city-data.com/articles/Minnesota.html',\n",
       " 'Missouri': 'https://www.city-data.com/articles/Missouri.html',\n",
       " 'Mississippi': 'https://www.city-data.com/articles/Mississippi.html',\n",
       " 'Montana': 'https://www.city-data.com/articles/Montana.html',\n",
       " 'North-Carolina': 'https://www.city-data.com/articles/North-Carolina.html',\n",
       " 'North-Dakota': 'https://www.city-data.com/articles/North-Dakota.html',\n",
       " 'Nebraska': 'https://www.city-data.com/articles/Nebraska.html',\n",
       " 'New-Hampshire': 'https://www.city-data.com/articles/New-Hampshire.html',\n",
       " 'New-Jersey': 'https://www.city-data.com/articles/New-Jersey.html',\n",
       " 'New-Mexico': 'https://www.city-data.com/articles/New-Mexico.html',\n",
       " 'Nevada': 'https://www.city-data.com/articles/Nevada.html',\n",
       " 'Nevada2': 'https://www.city-data.com/articles/Nevada2.html',\n",
       " 'New-York': 'https://www.city-data.com/articles/New-York.html',\n",
       " 'New-York2': 'https://www.city-data.com/articles/New-York2.html',\n",
       " 'New-York3': 'https://www.city-data.com/articles/New-York3.html',\n",
       " 'Ohio': 'https://www.city-data.com/articles/Ohio.html',\n",
       " 'Oklahoma': 'https://www.city-data.com/articles/Oklahoma.html',\n",
       " 'Oregon': 'https://www.city-data.com/articles/Oregon.html',\n",
       " 'Pennsylvania': 'https://www.city-data.com/articles/Pennsylvania.html',\n",
       " 'Pennsylvania2': 'https://www.city-data.com/articles/Pennsylvania2.html',\n",
       " 'Rhode-Island': 'https://www.city-data.com/articles/Rhode-Island.html',\n",
       " 'South-Carolina': 'https://www.city-data.com/articles/South-Carolina.html',\n",
       " 'South-Dakota': 'https://www.city-data.com/articles/South-Dakota.html',\n",
       " 'Tennessee': 'https://www.city-data.com/articles/Tennessee.html',\n",
       " 'Texas': 'https://www.city-data.com/articles/Texas.html',\n",
       " 'Texas2': 'https://www.city-data.com/articles/Texas2.html',\n",
       " 'Utah': 'https://www.city-data.com/articles/Utah.html',\n",
       " 'Virginia': 'https://www.city-data.com/articles/Virginia.html',\n",
       " 'Vermont': 'https://www.city-data.com/articles/Vermont.html',\n",
       " 'Washington': 'https://www.city-data.com/articles/Washington.html',\n",
       " 'Wisconsin': 'https://www.city-data.com/articles/Wisconsin.html',\n",
       " 'West-Virginia': 'https://www.city-data.com/articles/West-Virginia.html',\n",
       " 'Wyoming': 'https://www.city-data.com/articles/Wyoming.html'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba37d69516b96b2df66115ef342e83722e1a1518f5d2e0ee41eb08141fa2fe2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
